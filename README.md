# 備忘録

## モデル
症例報告で事前学習された[RoBERTa](https://huggingface.co/alabnii/jmedroberta-base-sentencepiece-vocab50000)を使用

## 設定
5分割交差検証（ファイル名でグループ）し、50エポック学習させ、検証データにおけるMicro-F1が最も高かったエポックのモデルでテストデータを評価

## 前処理
固有表現単位で分割し、それぞれに対してトーカナイズ（RWDに対する前処理とは異なる）

## Result (Micro-F1)

|Fold|RoBERTa|
|:---|---:|
|0 |0.733|
|1 |0.727|
|2 |0.775|
|3 |0.741|
|4 |0.751|
|Avg. |0.745|


## 文献
- 杉本海人, 壹岐太一, 知田悠生, 金沢輝一, 相澤彰子, JMedRoBERTa: 日本語の医学論文にもとづいた事前
